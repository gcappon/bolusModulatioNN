{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cappe/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Loading useful packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import sys\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#General purpose AI packages\n",
    "from sklearn.cross_validation import train_test_split,KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "\n",
    "#Keras packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, ActivityRegularization\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers \n",
    "\n",
    "#Hyperparameter optimization\n",
    "import hyperopt\n",
    "from hyperopt import hp, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## LOSSHISTORY CALLBACK CLASS ######################################\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATAFILE = os.path.join('data','data.csv')\n",
    "TARGETFILE = os.path.join('data','target.csv')\n",
    "OUTDIR = os.path.join('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## PREPARING DATA ##################################################\n",
    "dataset_trans = pd.read_table(os.path.join('data','dataset_trans.csv'),sep=',')\n",
    "target = np.asarray(dataset_trans['Y'])\n",
    "del dataset_trans['Y']\n",
    "del dataset_trans['min_risk']\n",
    "train = np.asarray(dataset_trans)\n",
    "train_val_size = 0.8 #80% training+validation set and 20% test set\n",
    "train_size = 0.7 #70% training set and 30% validation set\n",
    "X_tr_val, X_te, Y_tr_val, Y_te = train_test_split(train, target, train_size=train_val_size, random_state=1)\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X_tr_val, Y_tr_val, train_size=train_size, random_state=1)\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_te = scaler.transform(X_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform_int(name, lower, upper):\n",
    "    # `quniform` returns:\n",
    "    # round(uniform(low, high) / q) * q\n",
    "    return hp.quniform(name, lower, upper, q=1)\n",
    "\n",
    "def loguniform_int(name, lower, upper):\n",
    "    # Do not forget to make a logarithm for the\n",
    "    # lower and upper bounds.\n",
    "    return hp.qloguniform(name, np.log(lower), np.log(upper), q=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def train_nn(X_tr,Y_tr,X_val,Y_val,params,verbose,save):\n",
    "def train_nn(params):\n",
    "    \n",
    "    print('Testing: ', params)\n",
    "    verbose = 0\n",
    "    \n",
    "    #Build NN\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=int(params['n_nodes_1']), input_dim=np.shape(X_tr)[1], activity_regularizer=regularizers.l2(params['regularization_1'])))\n",
    "    model.add(Activation(params['activation_1']))\n",
    "    model.add(Dropout(params['dropout_1']))\n",
    "    model.add(Dense(units=int(params['n_nodes_2']),activity_regularizer=regularizers.l2(params['regularization_2'])))\n",
    "    model.add(Activation(params['activation_2']))\n",
    "    model.add(Dense(units=1))\n",
    "    opt = RMSprop(lr=params['opt_lr'], rho=params['opt_rho'], epsilon=params['opt_epsilon'], decay=params['opt_decay'])\n",
    "    model.compile(loss=params['comp_loss'],optimizer=opt)\n",
    "    \n",
    "    #Model callbacks\n",
    "    filepath = os.path.join('results','weights.best.hdf5')\n",
    "    mdlcheck = ModelCheckpoint(filepath, verbose=0, save_best_only=True)\n",
    "    mdllosses = LossHistory()\n",
    "    mdlstop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    #Model fit\n",
    "    n_epochs = 10000\n",
    "    n_batch = int(params['fit_n_batch'])\n",
    "    kf = KFold(n = np.shape(X_tr_val)[0], n_folds = 5)\n",
    "    performance_cv = []\n",
    "    #mdllosses_cv = []\n",
    "    models = []\n",
    "    \n",
    "    for tr_idx, val_idx in kf:\n",
    "        X_train, X_valid = X_tr_val[tr_idx], X_tr_val[val_idx]\n",
    "        Y_train, Y_valid = Y_tr_val[tr_idx], Y_tr_val[val_idx]\n",
    "        history = model.fit(X_train, Y_train, validation_data = (X_valid, Y_valid),  epochs = n_epochs, batch_size = n_batch, callbacks = [mdlstop,mdlcheck,mdllosses],verbose = verbose)\n",
    "        \n",
    "        #Recalling best weights and appending loss value and loss history\n",
    "        model.load_weights(filepath)\n",
    "        models.append(model)\n",
    "        performance_cv.append(min(mdllosses.val_losses))\n",
    "        #mdllosses_cv.append(mdllosses)\n",
    "        \n",
    "    #Calculating in-cv std \n",
    "    loss_std = np.std(performance_cv)\n",
    "    \n",
    "    print('Obtained loss: ', np.mean(performance_cv), ' (', loss_std, ')')\n",
    "    #Return model and best performances\n",
    "    return {'loss' : np.mean(performance_cv), 'status': STATUS_OK, 'model': models[np.argmin(performance_cv)], 'loss_std': loss_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training the NN... =====\n",
      "Testing:  {'activation_1': 'sigmoid', 'activation_2': 'sigmoid', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.32732630424811465, 'dropout_2': 0.49704415547836645, 'fit_n_batch': 98.0, 'n_nodes_1': 413.0, 'n_nodes_2': 978.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.313852928428758, 'regularization_2': 0.00867495042275418}\n",
      "Obtained loss:  104.268286593  ( 3.45157547884 )\n",
      "Testing:  {'activation_1': 'sigmoid', 'activation_2': 'sigmoid', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.11428433058926923, 'dropout_2': 0.11814049251348002, 'fit_n_batch': 88.0, 'n_nodes_1': 554.0, 'n_nodes_2': 629.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.3396579261583336, 'regularization_2': 0.1832017716266685}\n",
      "Obtained loss:  245.207883077  ( 7.75502058261 )\n",
      "Testing:  {'activation_1': 'relu', 'activation_2': 'sigmoid', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.009605583848120769, 'dropout_2': 0.08928553420070656, 'fit_n_batch': 108.0, 'n_nodes_1': 953.0, 'n_nodes_2': 892.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.4584098853833707, 'regularization_2': 0.05906820270094454}\n",
      "Obtained loss:  76.5643040073  ( 3.41676695019 )\n",
      "Testing:  {'activation_1': 'relu', 'activation_2': 'relu', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.4043314995863086, 'dropout_2': 0.04312191096675971, 'fit_n_batch': 101.0, 'n_nodes_1': 412.0, 'n_nodes_2': 994.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.35034656571333883, 'regularization_2': 0.2048839259416183}\n",
      "Obtained loss:  76.3820009956  ( 3.33507897685 )\n",
      "Testing:  {'activation_1': 'sigmoid', 'activation_2': 'sigmoid', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.373844988434755, 'dropout_2': 0.1577047330515322, 'fit_n_batch': 121.0, 'n_nodes_1': 384.0, 'n_nodes_2': 223.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.39014368909981956, 'regularization_2': 0.2201803099780817}\n",
      "Obtained loss:  147.204105915  ( 11.7002045201 )\n",
      "Testing:  {'activation_1': 'relu', 'activation_2': 'relu', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.09538822150295034, 'dropout_2': 0.4372208044975191, 'fit_n_batch': 48.0, 'n_nodes_1': 730.0, 'n_nodes_2': 544.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.25463560325534024, 'regularization_2': 0.17645672635214882}\n",
      "Obtained loss:  76.3811875187  ( 3.33528662977 )\n",
      "Testing:  {'activation_1': 'relu', 'activation_2': 'sigmoid', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.0445247334248548, 'dropout_2': 0.44098135878112527, 'fit_n_batch': 26.0, 'n_nodes_1': 227.0, 'n_nodes_2': 706.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.22589314418001039, 'regularization_2': 0.21446831737515487}\n",
      "Obtained loss:  76.3737433322  ( 3.33297015606 )\n",
      "Testing:  {'activation_1': 'relu', 'activation_2': 'sigmoid', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.2144110076834132, 'dropout_2': 0.4790173968571087, 'fit_n_batch': 32.0, 'n_nodes_1': 15.0, 'n_nodes_2': 950.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.35625746698903915, 'regularization_2': 0.18354835239620299}\n",
      "Obtained loss:  76.3804329398  ( 3.33036835028 )\n",
      "Testing:  {'activation_1': 'relu', 'activation_2': 'sigmoid', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.05118928504693471, 'dropout_2': 0.06656833610123652, 'fit_n_batch': 115.0, 'n_nodes_1': 210.0, 'n_nodes_2': 369.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.42167571297109263, 'regularization_2': 0.49632153638362614}\n",
      "Obtained loss:  76.4600370929  ( 3.31957316021 )\n",
      "Testing:  {'activation_1': 'sigmoid', 'activation_2': 'sigmoid', 'comp_loss': 'mean_absolute_error', 'dropout_1': 0.1876816324913766, 'dropout_2': 0.15868348141958177, 'fit_n_batch': 55.0, 'n_nodes_1': 894.0, 'n_nodes_2': 995.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0.0770693714789652, 'regularization_2': 0.337122654922676}\n",
      "Obtained loss:  588.322207279  ( 62.0195674201 )\n",
      "==============================\n",
      "\n",
      "======== Best NN... ========\n",
      "Validation loss:  76.37374333219074\n",
      "Best model hyperparameters:  {'activation_1': 0, 'activation_2': 1, 'dropout_1': 0.0445247334248548, 'dropout_2': 0.44098135878112527, 'fit_n_batch': 26.0, 'n_nodes_1': 227.0, 'n_nodes_2': 706.0, 'regularization_1': 0.22589314418001039, 'regularization_2': 0.21446831737515487}\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defining the trial memory space\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "#Defining the hyperparameter space\n",
    "parameter_space = {\n",
    "    'n_nodes_1': uniform_int('n_nodes_1', 1, 1000),\n",
    "    'regularization_1': hp.uniform('regularization_1',0,0.5),\n",
    "    'dropout_1': hp.uniform('dropout_1', 0, 0.5),\n",
    "    'activation_1': hp.choice('activation_1', ['relu','sigmoid']),\n",
    "    'n_nodes_2': uniform_int('n_nodes_2', 1, 1000),\n",
    "    'regularization_2': hp.uniform('regularization_2',0,0.5),\n",
    "    'dropout_2': hp.uniform('dropout_2', 0, 0.5),\n",
    "    'activation_2': hp.choice('activation_2', ['relu','sigmoid']),\n",
    "    'fit_n_batch' : uniform_int('fit_n_batch', 16, 128),\n",
    "    'comp_loss' : 'mean_squared_error',\n",
    "    'opt_lr' : 0.002,\n",
    "    'opt_rho' : 0.9,\n",
    "    'opt_epsilon' : 1e-08,\n",
    "    'opt_decay' : 0.0}\n",
    "\n",
    "#Defining the  tree\n",
    "tpe = hyperopt.partial(\n",
    "    hyperopt.tpe.suggest,\n",
    "\n",
    "    # Sample 1000 candidate and select candidate that\n",
    "    # has highest Expected Improvement (EI)\n",
    "    n_EI_candidates=1000,\n",
    "    \n",
    "    # Use 20% of best observations to estimate next\n",
    "    # set of parameters\n",
    "    gamma=0.2,\n",
    "    \n",
    "    # First 20 trials are going to be random\n",
    "    n_startup_jobs=20,\n",
    ")\n",
    "\n",
    "print('===== Training the NN... =====')\n",
    "best = hyperopt.fmin(\n",
    "    train_nn,\n",
    "    trials=trials,\n",
    "    space=parameter_space,\n",
    "\n",
    "    # Set up TPE for hyperparameter optimization\n",
    "    algo=tpe,\n",
    "\n",
    "    # Maximum number of iterations. Basically it trains at\n",
    "    # most 200 networks before choose the best one.\n",
    "    max_evals=10,\n",
    ")\n",
    "print('==============================\\n')\n",
    "\n",
    "\n",
    "#And the winner is...\n",
    "#trials.results <--- mi da la storia\n",
    "print('======== Best NN... ========')\n",
    "print('Validation loss: ', trials.best_trial['result']['loss'])\n",
    "print('Best model hyperparameters: ', best)\n",
    "model = trials.best_trial['result']['model']\n",
    "#loss_history = trials.best_trial['result']['loss_history']\n",
    "print('==============================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score NN:  9440.95326049\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5d82c8b0ea39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Plot train and validation losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'losses'"
     ]
    }
   ],
   "source": [
    "############## EVALUATING RESULTS  #############################################\n",
    "Y_te = np.squeeze(Y_te)\n",
    "Y_NN = np.squeeze(model.predict(X_te))\n",
    "\n",
    "#MSE\n",
    "print('\\n Score NN: ',mean_squared_error(Y_NN,Y_te))\n",
    "\n",
    "\"\"\"\n",
    "#Boxplot of the difference between actual values and estimates\n",
    "data_to_plot = [Y_te-Y_NN]\n",
    "plt.boxplot(data_to_plot)\n",
    "plt.show()\n",
    "\n",
    "#Histogram of the difference between actual values and estimates\n",
    "plt.hist(data_to_plot,bins=20)\n",
    "plt.show()\n",
    "\n",
    "#Plot of the actual values and estimates\n",
    "plt.plot(Y_te, marker='^')\n",
    "plt.plot(Y_NN, marker='o')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
