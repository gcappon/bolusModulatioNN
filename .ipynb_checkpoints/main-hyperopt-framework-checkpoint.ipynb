{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading useful packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import sys\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#General purpose AI packages\n",
    "from sklearn.cross_validation import train_test_split,KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "\n",
    "#Keras packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, ActivityRegularization\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers \n",
    "\n",
    "#Hyperparameter optimization\n",
    "import hyperopt\n",
    "from hyperopt import hp, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## LOSSHISTORY CALLBACK CLASS ######################################\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATAFILE = os.path.join('data','data.csv')\n",
    "TARGETFILE = os.path.join('data','target.csv')\n",
    "OUTDIR = os.path.join('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## PREPARING DATA ##################################################\n",
    "dataset_trans = pd.read_table(os.path.join('data','dataset_trans.csv'),sep=',')\n",
    "target = np.asarray(dataset_trans['Y'])\n",
    "del dataset_trans['Y']\n",
    "del dataset_trans['min_risk']\n",
    "train = np.asarray(dataset_trans)\n",
    "train_val_size = 0.8 #80% training+validation set and 20% test set\n",
    "train_size = 0.7 #70% training set and 30% validation set\n",
    "X_tr_val, X_te, Y_tr_val, Y_te = train_test_split(train, target, train_size=train_val_size, random_state=1)\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X_tr_val, Y_tr_val, train_size=train_size, random_state=1)\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_te = scaler.transform(X_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7970, 13)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_tr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform_int(name, lower, upper):\n",
    "    # `quniform` returns:\n",
    "    # round(uniform(low, high) / q) * q\n",
    "    return hp.quniform(name, lower, upper, q=1)\n",
    "\n",
    "def loguniform_int(name, lower, upper):\n",
    "    # Do not forget to make a logarithm for the\n",
    "    # lower and upper bounds.\n",
    "    return hp.qloguniform(name, np.log(lower), np.log(upper), q=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_nn(X_tr,Y_tr,X_val,Y_val,params,verbose,save):\n",
    "def train_nn(params):\n",
    "    \n",
    "    print('Testing: ', params)\n",
    "    verbose = 0\n",
    "    \n",
    "    #Build NN\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=int(params['n_nodes_1']), input_dim=np.shape(X_tr)[1], activity_regularizer=regularizers.l2(params['regularization_1'])))\n",
    "    model.add(Activation(params['activation_1']))\n",
    "    model.add(Dropout(params['dropout_1']))\n",
    "    model.add(Dense(units=int(params['n_nodes_2']),activity_regularizer=regularizers.l2(params['regularization_2'])))\n",
    "    model.add(Activation(params['activation_2']))\n",
    "    model.add(Dropout(params['dropout_2']))\n",
    "    model.add(Dense(units=int(params['n_nodes_3']),activity_regularizer=regularizers.l2(params['regularization_3'])))\n",
    "    model.add(Activation(params['activation_3']))\n",
    "    model.add(Dense(units=1))\n",
    "    opt = RMSprop(lr=params['opt_lr'], rho=params['opt_rho'], epsilon=params['opt_epsilon'], decay=params['opt_decay'])\n",
    "    model.compile(loss=params['comp_loss'],optimizer=opt)\n",
    "    \n",
    "    #Model callbacks\n",
    "    filepath = os.path.join('results','weights.best.hdf5')\n",
    "    mdlcheck = ModelCheckpoint(filepath, verbose=0, save_best_only=True)\n",
    "    mdllosses = LossHistory()\n",
    "    mdlstop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    #Model fit\n",
    "    n_epochs = 5000\n",
    "    n_batch = int(params['fit_n_batch'])\n",
    "    kf = KFold(n = np.shape(X_tr_val)[0], n_folds = 5)\n",
    "    performance_cv = []\n",
    "    #mdllosses_cv = []\n",
    "    models = []\n",
    "    \n",
    "    i = 1\n",
    "    for tr_idx, val_idx in kf:\n",
    "        print(\"Fold: \",i,\" of 5\")\n",
    "        i = i+1\n",
    "        X_train, X_valid = X_tr_val[tr_idx], X_tr_val[val_idx]\n",
    "        Y_train, Y_valid = Y_tr_val[tr_idx], Y_tr_val[val_idx]\n",
    "        history = model.fit(X_train, Y_train, validation_data = (X_valid, Y_valid),  epochs = n_epochs, batch_size = n_batch, callbacks = [mdlstop,mdlcheck,mdllosses],verbose = verbose)\n",
    "        \n",
    "        #Recalling best weights and appending loss value and loss history\n",
    "        model.load_weights(filepath)\n",
    "        models.append(model)\n",
    "        performance_cv.append(min(mdllosses.val_losses))\n",
    "        #mdllosses_cv.append(mdllosses)\n",
    "        \n",
    "    #Calculating in-cv std \n",
    "    loss_std = np.std(performance_cv)\n",
    "    \n",
    "    print('Obtained loss: ', np.mean(performance_cv), ' (', loss_std, ')')\n",
    "    #Return model and best performances\n",
    "    return {'loss' : np.mean(performance_cv), 'status': STATUS_OK, 'model': models[np.argmin(performance_cv)], 'loss_std': loss_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training the NN... =====\n",
      "Testing:  {'activation_1': 'tanh', 'activation_2': 'sigmoid', 'activation_3': 'sigmoid', 'comp_loss': 'mean_squared_error', 'dropout_1': 0.4801769970600562, 'dropout_2': 0.31918098273108614, 'fit_n_batch': 44.0, 'n_nodes_1': 438.0, 'n_nodes_2': 575.0, 'n_nodes_3': 928.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0, 'regularization_2': 0, 'regularization_3': 0}\n",
      "Fold:  1  of 5\n",
      "Fold:  2  of 5\n",
      "Fold:  3  of 5\n",
      "Fold:  4  of 5\n",
      "Fold:  5  of 5\n",
      "Obtained loss:  6080.87973284  ( 137.694422406 )\n",
      "Testing:  {'activation_1': 'tanh', 'activation_2': 'tanh', 'activation_3': 'relu', 'comp_loss': 'mean_squared_error', 'dropout_1': 0.2347875317792028, 'dropout_2': 0.38970478710493456, 'fit_n_batch': 120.0, 'n_nodes_1': 286.0, 'n_nodes_2': 649.0, 'n_nodes_3': 117.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0, 'regularization_2': 0, 'regularization_3': 0}\n",
      "Fold:  1  of 5\n",
      "Fold:  2  of 5\n",
      "Fold:  3  of 5\n",
      "Fold:  4  of 5\n",
      "Fold:  5  of 5\n",
      "Obtained loss:  5908.83262086  ( 445.252233012 )\n",
      "Testing:  {'activation_1': 'sigmoid', 'activation_2': 'tanh', 'activation_3': 'relu', 'comp_loss': 'mean_squared_error', 'dropout_1': 0.16481143399151527, 'dropout_2': 0.43301505958009284, 'fit_n_batch': 113.0, 'n_nodes_1': 614.0, 'n_nodes_2': 308.0, 'n_nodes_3': 564.0, 'opt_decay': 0.0, 'opt_epsilon': 1e-08, 'opt_lr': 0.002, 'opt_rho': 0.9, 'regularization_1': 0, 'regularization_2': 0, 'regularization_3': 0}\n",
      "Fold:  1  of 5\n",
      "Fold:  2  of 5\n",
      "Fold:  3  of 5\n",
      "Fold:  4  of 5\n",
      "Fold:  5  of 5\n",
      "Obtained loss:  5666.87758621  ( 419.813705325 )\n",
      "==============================\n",
      "\n",
      "======== Best NN... ========\n",
      "Validation loss:  5666.87758620584\n",
      "Best model hyperparameters:  {'activation_1': 1, 'activation_2': 2, 'activation_3': 0, 'dropout_1': 0.16481143399151527, 'dropout_2': 0.43301505958009284, 'fit_n_batch': 113.0, 'n_nodes_1': 614.0, 'n_nodes_2': 308.0, 'n_nodes_3': 564.0}\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defining the trial memory space\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "#Defining the hyperparameter space\n",
    "parameter_space = {\n",
    "    'n_nodes_1': uniform_int('n_nodes_1', 1, 1000),\n",
    "    'regularization_1': 0,\n",
    "    'dropout_1': hp.uniform('dropout_1', 0, 0.5),\n",
    "    'activation_1': hp.choice('activation_1', ['relu','sigmoid','tanh']),\n",
    "    'n_nodes_2': uniform_int('n_nodes_2', 1, 1000),\n",
    "    'regularization_2': 0,\n",
    "    'dropout_2': hp.uniform('dropout_2', 0, 0.5),\n",
    "    'activation_2': hp.choice('activation_2', ['relu','sigmoid','tanh']),\n",
    "    'n_nodes_3': uniform_int('n_nodes_3', 1, 1000),\n",
    "    'regularization_3': 0,\n",
    "    'activation_3': hp.choice('activation_3', ['relu','sigmoid','tanh']),\n",
    "    'fit_n_batch' : uniform_int('fit_n_batch', 16, 128),\n",
    "    'comp_loss' : 'mean_squared_error',\n",
    "    'opt_lr' : 0.002,\n",
    "    'opt_rho' : 0.9,\n",
    "    'opt_epsilon' : 1e-08,\n",
    "    'opt_decay' : 0.0}\n",
    "\n",
    "#Defining the  tree\n",
    "tpe = hyperopt.partial(\n",
    "    hyperopt.tpe.suggest,\n",
    "\n",
    "    # Sample 1000 candidate and select candidate that\n",
    "    # has highest Expected Improvement (EI)\n",
    "    n_EI_candidates=1000,\n",
    "    \n",
    "    # Use 20% of best observations to estimate next\n",
    "    # set of parameters\n",
    "    gamma=0.2,\n",
    "    \n",
    "    # First 20 trials are going to be random\n",
    "    n_startup_jobs=20,\n",
    ")\n",
    "\n",
    "print('===== Training the NN... =====')\n",
    "best = hyperopt.fmin(\n",
    "    train_nn,\n",
    "    trials=trials,\n",
    "    space=parameter_space,\n",
    "\n",
    "    # Set up TPE for hyperparameter optimization\n",
    "    algo=tpe,\n",
    "\n",
    "    # Maximum number of iterations. Basically it trains at\n",
    "    # most 200 networks before choose the best one.\n",
    "    max_evals=3,\n",
    ")\n",
    "print('==============================\\n')\n",
    "\n",
    "\n",
    "#And the winner is...\n",
    "#trials.results <--- mi da la storia\n",
    "print('======== Best NN... ========')\n",
    "print('Validation loss: ', trials.best_trial['result']['loss'])\n",
    "print('Best model hyperparameters: ', best)\n",
    "model = trials.best_trial['result']['model']\n",
    "#loss_history = trials.best_trial['result']['loss_history']\n",
    "print('==============================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score NN:  9440.95326049\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5d82c8b0ea39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Plot train and validation losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'losses'"
     ]
    }
   ],
   "source": [
    "############## EVALUATING RESULTS  #############################################\n",
    "Y_te = np.squeeze(Y_te)\n",
    "Y_NN = np.squeeze(model.predict(X_te))\n",
    "\n",
    "#MSE\n",
    "print('\\n Score NN: ',mean_squared_error(Y_NN,Y_te))\n",
    "\n",
    "\"\"\"\n",
    "#Boxplot of the difference between actual values and estimates\n",
    "data_to_plot = [Y_te-Y_NN]\n",
    "plt.boxplot(data_to_plot)\n",
    "plt.show()\n",
    "\n",
    "#Histogram of the difference between actual values and estimates\n",
    "plt.hist(data_to_plot,bins=20)\n",
    "plt.show()\n",
    "\n",
    "#Plot of the actual values and estimates\n",
    "plt.plot(Y_te, marker='^')\n",
    "plt.plot(Y_NN, marker='o')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
